{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement: Study Assistant for Quiz Question Generation\n",
    "\n",
    "In modern education, students often struggle to create effective quizzes for self-assessment. Manually generating relevant, diverse, and structured quiz questions can be time-consuming, leading to incomplete coverage of the material. This limitation hinders students' ability to effectively evaluate their understanding of complex topics.\n",
    "\n",
    "Existing solutions, such as Retrieval-Augmented Generation (RAG) systems, often depend on external databases or vector storage mechanisms, which can be resource-intensive. While these approaches are powerful, they are unnecessary for localized study scenarios. As a result, there is a need for a lightweight, standalone solution that leverages modern language models to efficiently generate quiz questions without relying on external dependencies.\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Develop a Study Assistant using LangChain that:\n",
    "\n",
    "Summarizes study material into concise points.\n",
    "Automatically generates multiple-choice quiz questions based on the summarized content.\n",
    "Functions without the need for external retrieval mechanisms or vector databases.\n",
    "The assistant should be user-friendly, adaptable to various educational topics, and designed to help students engage interactively with their learning material. This tool should be able to process study materials provided in course documents or any content the students choose to work with, and generate relevant quiz questions for review.\n",
    "\n",
    "## Instructions for the Course Project:\n",
    "\n",
    "### Using the Course Document:\n",
    "\n",
    "Students can use the study material provided in the course document to generate quiz questions. The document will contain educational content on a specific topic, which should be summarized into concise points. Based on the summary, the system should automatically generate multiple-choice questions with four options, including the correct answer.\n",
    "\n",
    "Loading Content into Prompts Using PyPDF2: PyPDF2 offers an efficient way to load study material from PDF documents. By using PyPDF2, you can extract the content from PDF files for further processing, such as summarizing the material and generating quiz questions.\n",
    "\n",
    "Example: If the document is a PDF, you can use PyPDF2 to extract the content.\n",
    "\n",
    "Example code to load content from a text file:\n",
    "\n",
    "!pip install PyPDF2\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "#### Open the PDF file\n",
    "with open(\"/content/Prompt Engineering.pdf\", \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "#### Extract text from all pages\n",
    "study_material = \"\"\n",
    "for page in reader.pages:\n",
    "    study_material += page.extract_text()\n",
    "\n",
    "#### Now 'study_material' contains the text from the PDF\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from IPython.display import display, Markdown, Image\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory, VectorStoreRetrieverMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableParallel, RunnableBranch\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, SQLChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from PROMPT ENGINEERING.PDF:\n",
      "What\n",
      "is\n",
      "Prompt\n",
      "Engineering?\n",
      "Prompt\n",
      "engineering\n",
      "is\n",
      "a\n",
      "practice\n",
      "within\n",
      "natural\n",
      "language\n",
      "processing\n",
      "(NLP)\n",
      "in\n",
      "artificial\n",
      "intelligence,\n",
      "where\n",
      "text\n",
      "is\n",
      "used\n",
      "to\n",
      "describe\n",
      "the\n",
      "task\n",
      "the\n",
      "AI\n",
      "should\n",
      "perform.\n",
      "Guided\n",
      "by\n",
      "this\n",
      "input,\n",
      "the\n",
      "AI\n",
      "generates\n",
      "an\n",
      "output,\n",
      "which\n",
      "could\n",
      "take\n",
      "various\n",
      "forms.\n",
      "The\n",
      "goal\n",
      "is\n",
      "to\n",
      "use\n",
      "human-understandable\n",
      "text\n",
      "to\n",
      "interact\n",
      "conversationally\n",
      "with\n",
      "models,\n",
      "allowing\n",
      "for\n",
      "flexibility\n",
      "in\n",
      "the\n",
      "model’ s\n",
      "performance\n",
      "due\n",
      "to\n",
      "the\n",
      "task\n",
      "description\n",
      "embedded\n",
      "in\n",
      "the\n",
      "prompt.\n",
      "What\n",
      "are\n",
      "Prompts?\n",
      "Prompts\n",
      "are\n",
      "detailed\n",
      "descriptions\n",
      "of\n",
      "the\n",
      "desired\n",
      "output\n",
      "from\n",
      "an\n",
      "AI\n",
      "model.\n",
      "They\n",
      "represent\n",
      "the\n",
      "interaction\n",
      "between\n",
      "the\n",
      "user\n",
      "and\n",
      "the\n",
      "model\n",
      "and\n",
      "help\n",
      "define\n",
      "what\n",
      "the\n",
      "AI\n",
      "is\n",
      "expected\n",
      "to\n",
      "do.\n",
      "The\n",
      "effectiveness\n",
      "of\n",
      "prompt\n",
      "engineering\n",
      "largely\n",
      "depends\n",
      "on\n",
      "how\n",
      "well\n",
      "the\n",
      "prompt\n",
      "is\n",
      "designed\n",
      "to\n",
      "guide\n",
      "the\n",
      "model.\n",
      "Examples\n",
      "of\n",
      "Prompt\n",
      "Engineering\n",
      "Prompts\n",
      "in\n",
      "large\n",
      "language\n",
      "models\n",
      "(LLMs)\n",
      "like\n",
      "ChatGPT\n",
      "or\n",
      "GPT -3\n",
      "can\n",
      "range\n",
      "from\n",
      "simple\n",
      "text\n",
      "queries\n",
      "to\n",
      "complex\n",
      "instructions.\n",
      "The\n",
      "key\n",
      "to\n",
      "effective\n",
      "prompting\n",
      "is\n",
      "providing\n",
      "sufficient\n",
      "detail.\n",
      "Examples\n",
      "of\n",
      "prompts\n",
      "for\n",
      "various\n",
      "tasks\n",
      "include:\n",
      "Text\n",
      "Prompts\n",
      "(ChatGPT,\n",
      "GPT):●\n",
      "\"What’ s\n",
      "the\n",
      "difference\n",
      "between\n",
      "generative\n",
      "AI\n",
      "and\n",
      "traditional\n",
      "AI?\"\n",
      "●\n",
      "\"Provide\n",
      "10\n",
      "variations\n",
      "for\n",
      "the\n",
      "headline,\n",
      "'Top\n",
      "generative\n",
      "AI\n",
      "use\n",
      "cases\n",
      "for\n",
      "the\n",
      "enterprise.'\"\n",
      "●\n",
      "\"Write\n",
      "an\n",
      "outline\n",
      "for\n",
      "an\n",
      "article\n",
      "on\n",
      "the\n",
      "benefits\n",
      "of\n",
      "generative\n",
      "AI\n",
      "for\n",
      "marketing.\"\n",
      "●\n",
      "\"Generate\n",
      "300\n",
      "words\n",
      "for\n",
      "each\n",
      "section\n",
      "of\n",
      "an\n",
      "article\n",
      "on\n",
      "the\n",
      "benefits\n",
      "of\n",
      "generative\n",
      "AI.\"\n",
      "●\n",
      "\"Craft\n",
      "a\n",
      "100-word\n",
      "product\n",
      "description\n",
      "for\n",
      "ProductXYZ\n",
      "in\n",
      "five\n",
      "styles.\"\n",
      "●\n",
      "\"Define\n",
      "types\n",
      "of\n",
      "prompt\n",
      "engineering\n",
      "basics\n",
      "in\n",
      "iambic\n",
      "pentameter ,\n",
      "Shakespearean\n",
      "style.\"\n",
      "Code\n",
      "Prompts\n",
      "(ChatGPT,\n",
      "Codex):\n",
      "●\n",
      "\"Act\n",
      "as\n",
      "an\n",
      "ASCII\n",
      "artist\n",
      "translating\n",
      "object\n",
      "names\n",
      "into\n",
      "ASCII\n",
      "code.\"\n",
      "●\n",
      "\"Identify\n",
      "mistakes\n",
      "in\n",
      "the\n",
      "following\n",
      "code.\"\n",
      "●\n",
      "\"Write\n",
      "a\n",
      "function\n",
      "to\n",
      "multiply\n",
      "two\n",
      "numbers\n",
      "and\n",
      "return\n",
      "the\n",
      "result.\"\n",
      "●\n",
      "\"Develop\n",
      "a\n",
      "basic\n",
      "REST\n",
      "API\n",
      "in\n",
      "Python.\"\n",
      "●\n",
      "\"Explain\n",
      "the\n",
      "function\n",
      "of\n",
      "this\n",
      "code\n",
      "snippet.\"\n",
      "●\n",
      "\"Simplify\n",
      "the\n",
      "following\n",
      "code.\"\n",
      "Image\n",
      "Prompts\n",
      "(Stable\n",
      "Diffusion,\n",
      "Midjourney,\n",
      "DALL-E\n",
      "2):\n",
      "●\n",
      "\"Depict\n",
      "a\n",
      "dog\n",
      "in\n",
      "a\n",
      "car\n",
      "wearing\n",
      "sunglasses\n",
      "and\n",
      "a\n",
      "hat\n",
      "in\n",
      "the\n",
      "style\n",
      "of\n",
      "Salvador\n",
      "Dali.\"\n",
      "●\n",
      "\"Illustrate\n",
      "a\n",
      "lizard\n",
      "on\n",
      "the\n",
      "beach\n",
      "in\n",
      "claymation\n",
      "art\n",
      "style.\"●\n",
      "\"Create\n",
      "an\n",
      "image\n",
      "of\n",
      "a\n",
      "man\n",
      "using\n",
      "a\n",
      "phone\n",
      "on\n",
      "the\n",
      "subway\n",
      "in\n",
      "4K\n",
      "resolution\n",
      "with\n",
      "bokeh\n",
      "blur.\"\n",
      "●\n",
      "\"Design\n",
      "a\n",
      "sticker\n",
      "illustration\n",
      "of\n",
      "a\n",
      "woman\n",
      "drinking\n",
      "coffee\n",
      "at\n",
      "a\n",
      "table\n",
      "with\n",
      "a\n",
      "checkered\n",
      "tablecloth.\"\n",
      "●\n",
      "\"Visualize\n",
      "a\n",
      "jungle\n",
      "forest\n",
      "with\n",
      "cinematic\n",
      "lighting\n",
      "and\n",
      "nature\n",
      "photography .\"\n",
      "●\n",
      "\"Generate\n",
      "a\n",
      "first-person\n",
      "view\n",
      "of\n",
      "looking\n",
      "out\n",
      "at\n",
      "orange\n",
      "clouds\n",
      "during\n",
      "sunrise.\"\n",
      "How\n",
      "to\n",
      "Engineer\n",
      "AI\n",
      "Prompts\n",
      "The\n",
      "quality\n",
      "of\n",
      "a\n",
      "prompt\n",
      "is\n",
      "critical.\n",
      "To\n",
      "improve\n",
      "prompt\n",
      "effectiveness,\n",
      "consider\n",
      "the\n",
      "following\n",
      "tips:\n",
      "●\n",
      "Role\n",
      "Playing\n",
      ":\n",
      "Make\n",
      "the\n",
      "model\n",
      "act\n",
      "as\n",
      "a\n",
      "specific\n",
      "entity\n",
      "(e.g.,\n",
      "teacher ,\n",
      "code\n",
      "editor ,\n",
      "interviewer)\n",
      "to\n",
      "tailor\n",
      "the\n",
      "interaction\n",
      "and\n",
      "target\n",
      "a\n",
      "specific\n",
      "outcome.\n",
      "●\n",
      "Clarity\n",
      ":\n",
      "Remove\n",
      "ambiguity\n",
      "by\n",
      "being\n",
      "concise.\n",
      "Avoid\n",
      "unnecessary\n",
      "details\n",
      "that\n",
      "might\n",
      "confuse\n",
      "the\n",
      "model.\n",
      "●\n",
      "Specification\n",
      ":\n",
      "Be\n",
      "specific\n",
      "in\n",
      "your\n",
      "instructions\n",
      "to\n",
      "direct\n",
      "the\n",
      "model’ s\n",
      "output\n",
      "clearly .\n",
      "●\n",
      "Consistency\n",
      ":\n",
      "Maintain\n",
      "a\n",
      "consistent\n",
      "tone\n",
      "and\n",
      "flow\n",
      "in\n",
      "the\n",
      "prompt\n",
      "to\n",
      "ensure\n",
      "coherent\n",
      "and\n",
      "legible\n",
      "responses.\n",
      "Elements\n",
      "of\n",
      "a\n",
      "Prompt\n",
      "The\n",
      "components\n",
      "that\n",
      "make\n",
      "up\n",
      "a\n",
      "prompt\n",
      "include:●\n",
      "Instruction\n",
      ":\n",
      "A\n",
      "statement\n",
      "telling\n",
      "the\n",
      "model\n",
      "what\n",
      "task\n",
      "to\n",
      "perform.\n",
      "●\n",
      "Context\n",
      ":\n",
      "Background\n",
      "information\n",
      "that\n",
      "helps\n",
      "the\n",
      "model\n",
      "understand\n",
      "the\n",
      "problem\n",
      "at\n",
      "hand.\n",
      "●\n",
      "Input\n",
      "Data\n",
      ":\n",
      "The\n",
      "input\n",
      "data\n",
      "given\n",
      "to\n",
      "the\n",
      "model\n",
      "to\n",
      "process.\n",
      "●\n",
      "Output\n",
      "Indicator\n",
      ":\n",
      "A\n",
      "specification\n",
      "of\n",
      "the\n",
      "expected\n",
      "output\n",
      "format\n",
      "(e.g.,\n",
      "code,\n",
      "text,\n",
      "image).\n",
      "Standard\n",
      "Prompt\n",
      "Patterns\n",
      "Prompts\n",
      "generally\n",
      "follow\n",
      "specific\n",
      "formats:\n",
      "User-Model\n",
      "Interaction\n",
      ":\n",
      "User:\n",
      "<Instruction>\n",
      "Model:\n",
      "<Response>\n",
      "Few-Shot\n",
      "Prompting\n",
      ":\n",
      "Provides\n",
      "a\n",
      "few\n",
      "examples\n",
      "of\n",
      "the\n",
      "task\n",
      "to\n",
      "guide\n",
      "the\n",
      "model.\n",
      "<Instruction>\n",
      "<Response><Instruction>\n",
      "<Response>\n",
      "<Instruction>\n",
      "<Response>\n",
      "Question-and-Answer\n",
      "Pattern\n",
      ":\n",
      "makefile\n",
      "Copy\n",
      "code\n",
      "Q:\n",
      "<Question>?\n",
      "A:\n",
      "<Answer>\n",
      "Q:\n",
      "<Question>?\n",
      "A:\n",
      "<Answer>●\n",
      "Prompting\n",
      "Techniques\n",
      "There\n",
      "are\n",
      "several\n",
      "advanced\n",
      "techniques\n",
      "for\n",
      "crafting\n",
      "effective\n",
      "prompts:\n",
      "1.\n",
      "Zero-Shot\n",
      "Prompting\n",
      ":\n",
      "The\n",
      "model\n",
      "performs\n",
      "a\n",
      "task\n",
      "without\n",
      "having\n",
      "been\n",
      "specifically\n",
      "trained\n",
      "on\n",
      "it,\n",
      "relying\n",
      "on\n",
      "its\n",
      "general\n",
      "knowledge.\n",
      "Example:\n",
      "Prompt:\n",
      "\"Classify\n",
      "the\n",
      "text\n",
      "into\n",
      "neutral,\n",
      "negative,\n",
      "or\n",
      "positive.\"\n",
      "Text:\n",
      "\"I\n",
      "think\n",
      "the\n",
      "presentation\n",
      "was\n",
      "awesome.\"\n",
      "Sentiment:\n",
      "Positive\n",
      "2.\n",
      "Few-Shot\n",
      "Prompting\n",
      "/\n",
      "In-Context\n",
      "Learning\n",
      ":\n",
      "The\n",
      "model\n",
      "is\n",
      "given\n",
      "a\n",
      "few\n",
      "examples\n",
      "to\n",
      "build\n",
      "upon,\n",
      "which\n",
      "helps\n",
      "it\n",
      "perform\n",
      "tasks\n",
      "more\n",
      "effectively .\n",
      "3.\n",
      "Chain-of-Thought\n",
      "(CoT)\n",
      ":\n",
      "This\n",
      "technique\n",
      "allows\n",
      "the\n",
      "model\n",
      "to\n",
      "process\n",
      "complex\n",
      "reasoning\n",
      "by\n",
      "breaking\n",
      "down\n",
      "the\n",
      "task\n",
      "into\n",
      "intermediate\n",
      "steps,\n",
      "improving\n",
      "output\n",
      "quality .\n",
      "What\n",
      "to\n",
      "Avoid\n",
      "When\n",
      "Creating\n",
      "Prompts\n",
      "Avoid\n",
      "the\n",
      "following\n",
      "pitfalls\n",
      "when\n",
      "creating\n",
      "prompts:\n",
      "●\n",
      "Information\n",
      "Overload\n",
      ":\n",
      "Too\n",
      "much\n",
      "detail\n",
      "can\n",
      "cause\n",
      "ambiguity\n",
      "and\n",
      "reduce\n",
      "the\n",
      "accuracy\n",
      "of\n",
      "responses.●\n",
      "Open-Ended\n",
      "Questions\n",
      ":\n",
      "Avoid\n",
      "vague\n",
      "or\n",
      "non-specific\n",
      "questions\n",
      "that\n",
      "may\n",
      "lead\n",
      "to\n",
      "imprecise\n",
      "responses.\n",
      "●\n",
      "Poor\n",
      "Use\n",
      "of\n",
      "Constraints\n",
      ":\n",
      "Avoid\n",
      "giving\n",
      "the\n",
      "model\n",
      "too\n",
      "much\n",
      "freedom.\n",
      "Instead,\n",
      "specify\n",
      "boundaries\n",
      "or\n",
      "requirements\n",
      "to\n",
      "guide\n",
      "the\n",
      "output\n",
      "more\n",
      "effectively .\n",
      "\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdfs(directory):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in the given directory.\n",
    "\n",
    "    Args:\n",
    "        directory: Path to the directory containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are file names and values are extracted text.\n",
    "    \"\"\"\n",
    "\n",
    "    text_from_pdfs = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"rb\") as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "            text_from_pdfs[filename] = text\n",
    "    return text_from_pdfs\n",
    "\n",
    "# Example usage:\n",
    "directory = \"./Langchain\"\n",
    "all_pdf_text = extract_text_from_pdfs(directory)\n",
    "\n",
    "# Print the extracted text from each PDF (optional)\n",
    "for filename, text in all_pdf_text.items():\n",
    "    print(f\"Text from {filename.upper()}:\\n{text}\\n\")\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt Engineering.pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "{\"input\":{\n",
    "    \"topic\" :\"Prompt Engineering for Agents\",\n",
    "    \"study_material\" :\"\"\"Prompt engineering involves designing and refining inputs to language models to achieve desired outputs.\n",
    "                         In the context of agents, prompt engineering allows for better control over how an agent interacts with the environment and solves specific tasks.\n",
    "                         This is particularly useful in domains like robotics and conversational AI.\n",
    "                         By adjusting the structure and content of the prompts, users can enhance an agent's performance on specific tasks.\"\"\"\n",
    "},\n",
    "\"output\":{\n",
    "    \"summary\": \"\"\"Prompt engineering refines inputs to language models for better output control.\n",
    "\n",
    "                  In agent-based systems, it helps control agent behavior and task performance.\n",
    "\n",
    "                  Useful in robotics and conversational AI.\"\"\",\n",
    "    \"Quiz_Question\" : \"\"\"What is the primary goal of prompt engineering in agent-based systems?\n",
    "\n",
    "                        a) To optimize agent memory\n",
    "                        b) To refine inputs for better output control\n",
    "                        c) To improve agent hardware\n",
    "                        d) To increase computational power\n",
    "                        \"\"\",\n",
    "\n",
    "    \"Answer\" : \"\"\"b) To refine inputs for better output control\"\"\",\n",
    "}\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Act as a study assistant for quiz generation.\n",
      "                    Go through the topic and text in the input thoroughly\n",
      "                    and generate output as shown in example_prompts.\n",
      "                    Also generate as many multiple choice questions as possible which would help the students in preparation.\n",
      "         \n",
      "         Donot hallucinate or make up any question that's not a part of the content\n",
      "Human: {'topic': 'Prompt Engineering for Agents', 'study_material': \"Prompt engineering involves designing and refining inputs to language models to achieve desired outputs.\\n                         In the context of agents, prompt engineering allows for better control over how an agent interacts with the environment and solves specific tasks.\\n                         This is particularly useful in domains like robotics and conversational AI.\\n                         By adjusting the structure and content of the prompts, users can enhance an agent's performance on specific tasks.\"}\n",
      "AI: {'summary': 'Prompt engineering refines inputs to language models for better output control.\\n\\n                  In agent-based systems, it helps control agent behavior and task performance.\\n\\n                  Useful in robotics and conversational AI.', 'Quiz_Question': 'What is the primary goal of prompt engineering in agent-based systems?\\n\\n                        a) To optimize agent memory\\n                        b) To refine inputs for better output control\\n                        c) To improve agent hardware\\n                        d) To increase computational power\\n                        ', 'Answer': 'b) To refine inputs for better output control'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"\"\"Act as a study assistant for quiz generation.\n",
    "                    Go through the topic and text in the input thoroughly\n",
    "                    and generate output as shown in example_prompts.\n",
    "                    Also generate as many multiple choice questions as possible which would help the students in preparation.\n",
    "         \n",
    "         Donot hallucinate or make up any question that's not a part of the content\"\"\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=few_shot_template,\n",
    "    examples=few_shot_examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"input\":{\"topic\": filename.replace(\".pdf\",\"\"),\"study_material\":text}} for filename, text in all_pdf_text.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': {'topic': 'Prompt Engineering',\n",
       "   'study_material': 'What\\nis\\nPrompt\\nEngineering?\\nPrompt\\nengineering\\nis\\na\\npractice\\nwithin\\nnatural\\nlanguage\\nprocessing\\n(NLP)\\nin\\nartificial\\nintelligence,\\nwhere\\ntext\\nis\\nused\\nto\\ndescribe\\nthe\\ntask\\nthe\\nAI\\nshould\\nperform.\\nGuided\\nby\\nthis\\ninput,\\nthe\\nAI\\ngenerates\\nan\\noutput,\\nwhich\\ncould\\ntake\\nvarious\\nforms.\\nThe\\ngoal\\nis\\nto\\nuse\\nhuman-understandable\\ntext\\nto\\ninteract\\nconversationally\\nwith\\nmodels,\\nallowing\\nfor\\nflexibility\\nin\\nthe\\nmodel’ s\\nperformance\\ndue\\nto\\nthe\\ntask\\ndescription\\nembedded\\nin\\nthe\\nprompt.\\nWhat\\nare\\nPrompts?\\nPrompts\\nare\\ndetailed\\ndescriptions\\nof\\nthe\\ndesired\\noutput\\nfrom\\nan\\nAI\\nmodel.\\nThey\\nrepresent\\nthe\\ninteraction\\nbetween\\nthe\\nuser\\nand\\nthe\\nmodel\\nand\\nhelp\\ndefine\\nwhat\\nthe\\nAI\\nis\\nexpected\\nto\\ndo.\\nThe\\neffectiveness\\nof\\nprompt\\nengineering\\nlargely\\ndepends\\non\\nhow\\nwell\\nthe\\nprompt\\nis\\ndesigned\\nto\\nguide\\nthe\\nmodel.\\nExamples\\nof\\nPrompt\\nEngineering\\nPrompts\\nin\\nlarge\\nlanguage\\nmodels\\n(LLMs)\\nlike\\nChatGPT\\nor\\nGPT -3\\ncan\\nrange\\nfrom\\nsimple\\ntext\\nqueries\\nto\\ncomplex\\ninstructions.\\nThe\\nkey\\nto\\neffective\\nprompting\\nis\\nproviding\\nsufficient\\ndetail.\\nExamples\\nof\\nprompts\\nfor\\nvarious\\ntasks\\ninclude:\\nText\\nPrompts\\n(ChatGPT,\\nGPT):●\\n\"What’ s\\nthe\\ndifference\\nbetween\\ngenerative\\nAI\\nand\\ntraditional\\nAI?\"\\n●\\n\"Provide\\n10\\nvariations\\nfor\\nthe\\nheadline,\\n\\'Top\\ngenerative\\nAI\\nuse\\ncases\\nfor\\nthe\\nenterprise.\\'\"\\n●\\n\"Write\\nan\\noutline\\nfor\\nan\\narticle\\non\\nthe\\nbenefits\\nof\\ngenerative\\nAI\\nfor\\nmarketing.\"\\n●\\n\"Generate\\n300\\nwords\\nfor\\neach\\nsection\\nof\\nan\\narticle\\non\\nthe\\nbenefits\\nof\\ngenerative\\nAI.\"\\n●\\n\"Craft\\na\\n100-word\\nproduct\\ndescription\\nfor\\nProductXYZ\\nin\\nfive\\nstyles.\"\\n●\\n\"Define\\ntypes\\nof\\nprompt\\nengineering\\nbasics\\nin\\niambic\\npentameter ,\\nShakespearean\\nstyle.\"\\nCode\\nPrompts\\n(ChatGPT,\\nCodex):\\n●\\n\"Act\\nas\\nan\\nASCII\\nartist\\ntranslating\\nobject\\nnames\\ninto\\nASCII\\ncode.\"\\n●\\n\"Identify\\nmistakes\\nin\\nthe\\nfollowing\\ncode.\"\\n●\\n\"Write\\na\\nfunction\\nto\\nmultiply\\ntwo\\nnumbers\\nand\\nreturn\\nthe\\nresult.\"\\n●\\n\"Develop\\na\\nbasic\\nREST\\nAPI\\nin\\nPython.\"\\n●\\n\"Explain\\nthe\\nfunction\\nof\\nthis\\ncode\\nsnippet.\"\\n●\\n\"Simplify\\nthe\\nfollowing\\ncode.\"\\nImage\\nPrompts\\n(Stable\\nDiffusion,\\nMidjourney,\\nDALL-E\\n2):\\n●\\n\"Depict\\na\\ndog\\nin\\na\\ncar\\nwearing\\nsunglasses\\nand\\na\\nhat\\nin\\nthe\\nstyle\\nof\\nSalvador\\nDali.\"\\n●\\n\"Illustrate\\na\\nlizard\\non\\nthe\\nbeach\\nin\\nclaymation\\nart\\nstyle.\"●\\n\"Create\\nan\\nimage\\nof\\na\\nman\\nusing\\na\\nphone\\non\\nthe\\nsubway\\nin\\n4K\\nresolution\\nwith\\nbokeh\\nblur.\"\\n●\\n\"Design\\na\\nsticker\\nillustration\\nof\\na\\nwoman\\ndrinking\\ncoffee\\nat\\na\\ntable\\nwith\\na\\ncheckered\\ntablecloth.\"\\n●\\n\"Visualize\\na\\njungle\\nforest\\nwith\\ncinematic\\nlighting\\nand\\nnature\\nphotography .\"\\n●\\n\"Generate\\na\\nfirst-person\\nview\\nof\\nlooking\\nout\\nat\\norange\\nclouds\\nduring\\nsunrise.\"\\nHow\\nto\\nEngineer\\nAI\\nPrompts\\nThe\\nquality\\nof\\na\\nprompt\\nis\\ncritical.\\nTo\\nimprove\\nprompt\\neffectiveness,\\nconsider\\nthe\\nfollowing\\ntips:\\n●\\nRole\\nPlaying\\n:\\nMake\\nthe\\nmodel\\nact\\nas\\na\\nspecific\\nentity\\n(e.g.,\\nteacher ,\\ncode\\neditor ,\\ninterviewer)\\nto\\ntailor\\nthe\\ninteraction\\nand\\ntarget\\na\\nspecific\\noutcome.\\n●\\nClarity\\n:\\nRemove\\nambiguity\\nby\\nbeing\\nconcise.\\nAvoid\\nunnecessary\\ndetails\\nthat\\nmight\\nconfuse\\nthe\\nmodel.\\n●\\nSpecification\\n:\\nBe\\nspecific\\nin\\nyour\\ninstructions\\nto\\ndirect\\nthe\\nmodel’ s\\noutput\\nclearly .\\n●\\nConsistency\\n:\\nMaintain\\na\\nconsistent\\ntone\\nand\\nflow\\nin\\nthe\\nprompt\\nto\\nensure\\ncoherent\\nand\\nlegible\\nresponses.\\nElements\\nof\\na\\nPrompt\\nThe\\ncomponents\\nthat\\nmake\\nup\\na\\nprompt\\ninclude:●\\nInstruction\\n:\\nA\\nstatement\\ntelling\\nthe\\nmodel\\nwhat\\ntask\\nto\\nperform.\\n●\\nContext\\n:\\nBackground\\ninformation\\nthat\\nhelps\\nthe\\nmodel\\nunderstand\\nthe\\nproblem\\nat\\nhand.\\n●\\nInput\\nData\\n:\\nThe\\ninput\\ndata\\ngiven\\nto\\nthe\\nmodel\\nto\\nprocess.\\n●\\nOutput\\nIndicator\\n:\\nA\\nspecification\\nof\\nthe\\nexpected\\noutput\\nformat\\n(e.g.,\\ncode,\\ntext,\\nimage).\\nStandard\\nPrompt\\nPatterns\\nPrompts\\ngenerally\\nfollow\\nspecific\\nformats:\\nUser-Model\\nInteraction\\n:\\nUser:\\n<Instruction>\\nModel:\\n<Response>\\nFew-Shot\\nPrompting\\n:\\nProvides\\na\\nfew\\nexamples\\nof\\nthe\\ntask\\nto\\nguide\\nthe\\nmodel.\\n<Instruction>\\n<Response><Instruction>\\n<Response>\\n<Instruction>\\n<Response>\\nQuestion-and-Answer\\nPattern\\n:\\nmakefile\\nCopy\\ncode\\nQ:\\n<Question>?\\nA:\\n<Answer>\\nQ:\\n<Question>?\\nA:\\n<Answer>●\\nPrompting\\nTechniques\\nThere\\nare\\nseveral\\nadvanced\\ntechniques\\nfor\\ncrafting\\neffective\\nprompts:\\n1.\\nZero-Shot\\nPrompting\\n:\\nThe\\nmodel\\nperforms\\na\\ntask\\nwithout\\nhaving\\nbeen\\nspecifically\\ntrained\\non\\nit,\\nrelying\\non\\nits\\ngeneral\\nknowledge.\\nExample:\\nPrompt:\\n\"Classify\\nthe\\ntext\\ninto\\nneutral,\\nnegative,\\nor\\npositive.\"\\nText:\\n\"I\\nthink\\nthe\\npresentation\\nwas\\nawesome.\"\\nSentiment:\\nPositive\\n2.\\nFew-Shot\\nPrompting\\n/\\nIn-Context\\nLearning\\n:\\nThe\\nmodel\\nis\\ngiven\\na\\nfew\\nexamples\\nto\\nbuild\\nupon,\\nwhich\\nhelps\\nit\\nperform\\ntasks\\nmore\\neffectively .\\n3.\\nChain-of-Thought\\n(CoT)\\n:\\nThis\\ntechnique\\nallows\\nthe\\nmodel\\nto\\nprocess\\ncomplex\\nreasoning\\nby\\nbreaking\\ndown\\nthe\\ntask\\ninto\\nintermediate\\nsteps,\\nimproving\\noutput\\nquality .\\nWhat\\nto\\nAvoid\\nWhen\\nCreating\\nPrompts\\nAvoid\\nthe\\nfollowing\\npitfalls\\nwhen\\ncreating\\nprompts:\\n●\\nInformation\\nOverload\\n:\\nToo\\nmuch\\ndetail\\ncan\\ncause\\nambiguity\\nand\\nreduce\\nthe\\naccuracy\\nof\\nresponses.●\\nOpen-Ended\\nQuestions\\n:\\nAvoid\\nvague\\nor\\nnon-specific\\nquestions\\nthat\\nmay\\nlead\\nto\\nimprecise\\nresponses.\\n●\\nPoor\\nUse\\nof\\nConstraints\\n:\\nAvoid\\ngiving\\nthe\\nmodel\\ntoo\\nmuch\\nfreedom.\\nInstead,\\nspecify\\nboundaries\\nor\\nrequirements\\nto\\nguide\\nthe\\noutput\\nmore\\neffectively .'}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    few_shot_prompt\n",
    "          |\n",
    "        chatgpt\n",
    "          |\n",
    "    StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.map().invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Summary:</span>                                                                                                           \n",
       "Prompt engineering involves designing and refining inputs to language models to achieve desired outputs. In the    \n",
       "context of agents, it allows for better control over how an agent interacts with the environment and solves        \n",
       "specific tasks. This is particularly useful in domains like robotics and conversational AI. By adjusting the       \n",
       "structure and content of the prompts, users can enhance an agent's performance on specific tasks.                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "<span style=\"font-weight: bold\">Quiz Questions:</span>                                                                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">What is prompt engineering primarily concerned with?</span>                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) Designing hardware for agents                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) Refining inputs to language models                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) Increasing the speed of language models                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) Developing new programming languages                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> b) Refining inputs to language models                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">In which domains is prompt engineering particularly useful?</span>                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) Web development and data analysis                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) Robotics and conversational AI                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) Graphic design and video editing                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) Network security and database management                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> b) Robotics and conversational AI                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">How does prompt engineering affect an agent's performance?</span>                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) It has no effect on performance                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) It enhances performance on specific tasks                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) It complicates the agent's decision-making process                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) It reduces the agent's ability to learn                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> b) It enhances performance on specific tasks                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">What is one of the main benefits of adjusting the structure and content of prompts?</span>                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) It makes the agent more complex                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) It allows for better control over agent interactions                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) It decreases the agent's efficiency                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) It eliminates the need for training data                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> b) It allows for better control over agent interactions                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">What does prompt engineering allow users to do in the context of agents?</span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) Increase the agent's physical capabilities                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) Control how the agent interacts with the environment                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) Change the agent's programming language                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) Reduce the agent's operational costs                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> b) Control how the agent interacts with the environment                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 6 </span><span style=\"font-weight: bold\">Which of the following best describes the relationship between prompt engineering and task performance?</span>         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>a) Prompt engineering has no relationship with task performance                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>b) Prompt engineering can hinder task performance                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>c) Prompt engineering can enhance task performance                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>d) Prompt engineering is only relevant for non-agent systems                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Answer:</span> c) Prompt engineering can enhance task performance                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "These questions should help students prepare effectively for their understanding of prompt engineering in the      \n",
       "context of agents.                                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSummary:\u001b[0m                                                                                                           \n",
       "Prompt engineering involves designing and refining inputs to language models to achieve desired outputs. In the    \n",
       "context of agents, it allows for better control over how an agent interacts with the environment and solves        \n",
       "specific tasks. This is particularly useful in domains like robotics and conversational AI. By adjusting the       \n",
       "structure and content of the prompts, users can enhance an agent's performance on specific tasks.                  \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\u001b[1mQuiz Questions:\u001b[0m                                                                                                    \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mWhat is prompt engineering primarily concerned with?\u001b[0m                                                            \n",
       "\u001b[1;33m   \u001b[0ma) Designing hardware for agents                                                                                \n",
       "\u001b[1;33m   \u001b[0mb) Refining inputs to language models                                                                           \n",
       "\u001b[1;33m   \u001b[0mc) Increasing the speed of language models                                                                      \n",
       "\u001b[1;33m   \u001b[0md) Developing new programming languages                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m b) Refining inputs to language models                                                                   \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mIn which domains is prompt engineering particularly useful?\u001b[0m                                                     \n",
       "\u001b[1;33m   \u001b[0ma) Web development and data analysis                                                                            \n",
       "\u001b[1;33m   \u001b[0mb) Robotics and conversational AI                                                                               \n",
       "\u001b[1;33m   \u001b[0mc) Graphic design and video editing                                                                             \n",
       "\u001b[1;33m   \u001b[0md) Network security and database management                                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m b) Robotics and conversational AI                                                                       \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1mHow does prompt engineering affect an agent's performance?\u001b[0m                                                      \n",
       "\u001b[1;33m   \u001b[0ma) It has no effect on performance                                                                              \n",
       "\u001b[1;33m   \u001b[0mb) It enhances performance on specific tasks                                                                    \n",
       "\u001b[1;33m   \u001b[0mc) It complicates the agent's decision-making process                                                           \n",
       "\u001b[1;33m   \u001b[0md) It reduces the agent's ability to learn                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m b) It enhances performance on specific tasks                                                            \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1mWhat is one of the main benefits of adjusting the structure and content of prompts?\u001b[0m                             \n",
       "\u001b[1;33m   \u001b[0ma) It makes the agent more complex                                                                              \n",
       "\u001b[1;33m   \u001b[0mb) It allows for better control over agent interactions                                                         \n",
       "\u001b[1;33m   \u001b[0mc) It decreases the agent's efficiency                                                                          \n",
       "\u001b[1;33m   \u001b[0md) It eliminates the need for training data                                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m b) It allows for better control over agent interactions                                                 \n",
       "\u001b[1;33m 5 \u001b[0m\u001b[1mWhat does prompt engineering allow users to do in the context of agents?\u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0ma) Increase the agent's physical capabilities                                                                   \n",
       "\u001b[1;33m   \u001b[0mb) Control how the agent interacts with the environment                                                         \n",
       "\u001b[1;33m   \u001b[0mc) Change the agent's programming language                                                                      \n",
       "\u001b[1;33m   \u001b[0md) Reduce the agent's operational costs                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m b) Control how the agent interacts with the environment                                                 \n",
       "\u001b[1;33m 6 \u001b[0m\u001b[1mWhich of the following best describes the relationship between prompt engineering and task performance?\u001b[0m         \n",
       "\u001b[1;33m   \u001b[0ma) Prompt engineering has no relationship with task performance                                                 \n",
       "\u001b[1;33m   \u001b[0mb) Prompt engineering can hinder task performance                                                               \n",
       "\u001b[1;33m   \u001b[0mc) Prompt engineering can enhance task performance                                                              \n",
       "\u001b[1;33m   \u001b[0md) Prompt engineering is only relevant for non-agent systems                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1mAnswer:\u001b[0m c) Prompt engineering can enhance task performance                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "These questions should help students prepare effectively for their understanding of prompt engineering in the      \n",
       "context of agents.                                                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
